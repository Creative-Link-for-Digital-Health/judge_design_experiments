{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eedf9f23",
   "metadata": {},
   "source": [
    "# Who is Judging the Judges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa91994",
   "metadata": {},
   "source": [
    "### Binary Classification Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, cohen_kappa_score, mean_absolute_error, mean_squared_error\n",
    ")\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "expert_df = pd.read_csv('expert_eval/expert_eval_20250622_230830.csv')\n",
    "judge_a_df = pd.read_csv('llm_eval/binary_class_judge_output_A.csv')\n",
    "judge_b_df = pd.read_csv('llm_eval/binary_class_judge_output_B.csv')\n",
    "\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"- Expert evaluations: {len(expert_df)} rows\")\n",
    "print(f\"- Judge A predictions: {len(judge_a_df)} rows\") \n",
    "print(f\"- Judge B predictions: {len(judge_b_df)} rows\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7449d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing - ensure boolean values are properly handled\n",
    "def convert_to_bool(df, col_name):\n",
    "    \"\"\"Convert string boolean values to actual booleans\"\"\"\n",
    "    if df[col_name].dtype == 'object':\n",
    "        df[col_name] = df[col_name].map({'True': True, 'False': False, True: True, False: False})\n",
    "    return df\n",
    "\n",
    "expert_df = convert_to_bool(expert_df, 'evaluation')\n",
    "judge_a_df = convert_to_bool(judge_a_df, 'label')\n",
    "judge_b_df = convert_to_bool(judge_b_df, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data alignment by turn_id\n",
    "print(\"Verifying data alignment...\")\n",
    "assert all(expert_df['turn_id'].sort_values() == judge_a_df['turn_id'].sort_values()), \"Turn IDs don't match between expert and Judge A\"\n",
    "assert all(expert_df['turn_id'].sort_values() == judge_b_df['turn_id'].sort_values()), \"Turn IDs don't match between expert and Judge B\"\n",
    "print(\"✓ All datasets have matching turn_ids\")\n",
    "print()\n",
    "\n",
    "# Sort all dataframes by turn_id for proper alignment\n",
    "expert_df = expert_df.sort_values('turn_id').reset_index(drop=True)\n",
    "judge_a_df = judge_a_df.sort_values('turn_id').reset_index(drop=True)\n",
    "judge_b_df = judge_b_df.sort_values('turn_id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all dataframes by turn_id for proper alignment\n",
    "expert_df = expert_df.sort_values('turn_id').reset_index(drop=True)\n",
    "judge_a_df = judge_a_df.sort_values('turn_id').reset_index(drop=True)\n",
    "judge_b_df = judge_b_df.sort_values('turn_id').reset_index(drop=True)\n",
    "\n",
    "# Extract ground truth and predictions\n",
    "y_true = expert_df['evaluation'].values\n",
    "y_pred_a = judge_a_df['label'].values\n",
    "y_pred_b = judge_b_df['label'].values\n",
    "\n",
    "print(\"Class Distribution in Ground Truth:\")\n",
    "print(f\"True (Positive): {sum(y_true)} ({sum(y_true)/len(y_true)*100:.1f}%)\")\n",
    "print(f\"False (Negative): {len(y_true) - sum(y_true)} ({(len(y_true) - sum(y_true))/len(y_true)*100:.1f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637974bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, judge_name):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Inter-rater agreement\n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # Simple agreement rate\n",
    "    metrics['agreement_rate'] = np.mean(y_true == y_pred)\n",
    "    \n",
    "    # Confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['true_positives'] = tp\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_negatives'] = fn\n",
    "    \n",
    "    # Specificity (True Negative Rate)\n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for both judges\n",
    "metrics_a = calculate_metrics(y_true, y_pred_a, \"Judge A\")\n",
    "metrics_b = calculate_metrics(y_true, y_pred_b, \"Judge B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a498471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Cohen\\'s Kappa', 'Specificity'],\n",
    "    'Judge A': [\n",
    "        f\"{metrics_a['accuracy']:.3f}\",\n",
    "        f\"{metrics_a['precision']:.3f}\",\n",
    "        f\"{metrics_a['recall']:.3f}\",\n",
    "        f\"{metrics_a['f1']:.3f}\",\n",
    "        f\"{metrics_a['cohen_kappa']:.3f}\",\n",
    "        f\"{metrics_a['specificity']:.3f}\"\n",
    "    ],\n",
    "    'Judge B': [\n",
    "        f\"{metrics_b['accuracy']:.3f}\",\n",
    "        f\"{metrics_b['precision']:.3f}\",\n",
    "        f\"{metrics_b['recall']:.3f}\",\n",
    "        f\"{metrics_b['f1']:.3f}\",\n",
    "        f\"{metrics_b['cohen_kappa']:.3f}\",\n",
    "        f\"{metrics_b['specificity']:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"DETAILED BREAKDOWN\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for judge_name, metrics in [(\"Judge A\", metrics_a), (\"Judge B\", metrics_b)]:\n",
    "    print(f\"\\n{judge_name}:\")\n",
    "    print(f\"  Agreement Rate: {metrics['agreement_rate']:.1%}\")\n",
    "    print(f\"  True Positives:  {metrics['true_positives']}\")\n",
    "    print(f\"  False Positives: {metrics['false_positives']}\")\n",
    "    print(f\"  True Negatives:  {metrics['true_negatives']}\")\n",
    "    print(f\"  False Negatives: {metrics['false_negatives']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ddf56",
   "metadata": {},
   "source": [
    "### Continuous Scale Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4dd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "expert_df = pd.read_csv('expert_eval/likert_eval_20250623_081631.csv')\n",
    "judge_a_df = pd.read_csv('llm_eval/continuous_judge_output_A.csv')\n",
    "judge_b_df = pd.read_csv('llm_eval/continuous_judge_output_B.csv')\n",
    "\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"- Expert Likert evaluations: {len(expert_df)} rows\")\n",
    "print(f\"- Judge A predictions: {len(judge_a_df)} rows\") \n",
    "print(f\"- Judge B predictions: {len(judge_b_df)} rows\")\n",
    "print()\n",
    "\n",
    "# Verify data alignment\n",
    "print(\"Verifying data alignment...\")\n",
    "assert all(expert_df['turn_id'].sort_values() == judge_a_df['turn_id'].sort_values()), \"Turn IDs don't match between expert and Judge A\"\n",
    "assert all(expert_df['turn_id'].sort_values() == judge_b_df['turn_id'].sort_values()), \"Turn IDs don't match between expert and Judge B\"\n",
    "print(\"✓ All datasets have matching turn_ids\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all dataframes by turn_id for proper alignment\n",
    "expert_df = expert_df.sort_values('turn_id').reset_index(drop=True)\n",
    "judge_a_df = judge_a_df.sort_values('turn_id').reset_index(drop=True)\n",
    "judge_b_df = judge_b_df.sort_values('turn_id').reset_index(drop=True)\n",
    "\n",
    "# Extract ratings\n",
    "expert_ratings = expert_df['likert_rating'].values\n",
    "judge_a_ratings = judge_a_df['label'].values\n",
    "judge_b_ratings = judge_b_df['label'].values\n",
    "\n",
    "# Validate scale range\n",
    "print(\"Rating Scale Validation:\")\n",
    "print(f\"Expert ratings range: {expert_ratings.min()} - {expert_ratings.max()}\")\n",
    "print(f\"Judge A ratings range: {judge_a_ratings.min()} - {judge_a_ratings.max()}\")\n",
    "print(f\"Judge B ratings range: {judge_b_ratings.min()} - {judge_b_ratings.max()}\")\n",
    "\n",
    "# Check if all ratings are within 1-5 scale\n",
    "all_ratings = np.concatenate([expert_ratings, judge_a_ratings, judge_b_ratings])\n",
    "if not all((1 <= rating <= 5) for rating in all_ratings):\n",
    "    print(\"⚠ WARNING: Some ratings are outside the 1-5 Likert scale range!\")\n",
    "else:\n",
    "    print(\"✓ All ratings are within the expected 1-5 Likert scale\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RATING DISTRIBUTION ANALYSIS ===\n",
    "\n",
    "print(\"RATING DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def analyze_distribution(ratings, name):\n",
    "    \"\"\"Analyze the distribution of ratings\"\"\"\n",
    "    distribution = pd.Series(ratings).value_counts().sort_index()\n",
    "    print(f\"\\n{name} Distribution:\")\n",
    "    for rating in range(1, 6):\n",
    "        count = distribution.get(rating, 0)\n",
    "        percentage = (count / len(ratings)) * 100\n",
    "        print(f\"  {rating}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"  Mean: {np.mean(ratings):.2f}\")\n",
    "    print(f\"  Median: {np.median(ratings):.1f}\")\n",
    "    print(f\"  Std Dev: {np.std(ratings):.2f}\")\n",
    "    return distribution\n",
    "\n",
    "expert_dist = analyze_distribution(expert_ratings, \"Expert\")\n",
    "judge_a_dist = analyze_distribution(judge_a_ratings, \"Judge A\")\n",
    "judge_b_dist = analyze_distribution(judge_b_ratings, \"Judge B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORRELATION ANALYSIS ===\n",
    "\n",
    "print(\"\\n\\nCORRELATION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def calculate_correlations(ground_truth, predictions, judge_name):\n",
    "    \"\"\"Calculate various correlation metrics\"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    # Pearson correlation (linear relationship)\n",
    "    pearson_r, pearson_p = pearsonr(ground_truth, predictions)\n",
    "    correlations['pearson_r'] = pearson_r\n",
    "    correlations['pearson_p'] = pearson_p\n",
    "    \n",
    "    # Spearman correlation (monotonic relationship)\n",
    "    spearman_r, spearman_p = spearmanr(ground_truth, predictions)\n",
    "    correlations['spearman_r'] = spearman_r\n",
    "    correlations['spearman_p'] = spearman_p\n",
    "    \n",
    "    # Kendall's tau (rank-based, handles ties well)\n",
    "    kendall_tau, kendall_p = kendalltau(ground_truth, predictions)\n",
    "    correlations['kendall_tau'] = kendall_tau\n",
    "    correlations['kendall_p'] = kendall_p\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "corr_a = calculate_correlations(expert_ratings, judge_a_ratings, \"Judge A\")\n",
    "corr_b = calculate_correlations(expert_ratings, judge_b_ratings, \"Judge B\")\n",
    "\n",
    "# Create correlation comparison table\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Metric': ['Pearson r', 'Spearman ρ', 'Kendall τ'],\n",
    "    'Judge A': [\n",
    "        f\"{corr_a['pearson_r']:.3f} (p={corr_a['pearson_p']:.3f})\",\n",
    "        f\"{corr_a['spearman_r']:.3f} (p={corr_a['spearman_p']:.3f})\",\n",
    "        f\"{corr_a['kendall_tau']:.3f} (p={corr_a['kendall_p']:.3f})\"\n",
    "    ],\n",
    "    'Judge B': [\n",
    "        f\"{corr_b['pearson_r']:.3f} (p={corr_b['pearson_p']:.3f})\",\n",
    "        f\"{corr_b['spearman_r']:.3f} (p={corr_b['spearman_p']:.3f})\",\n",
    "        f\"{corr_b['kendall_tau']:.3f} (p={corr_b['kendall_p']:.3f})\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Correlation with Expert Ratings:\")\n",
    "print(correlation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75639cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8317478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ERROR METRICS ===\n",
    "\n",
    "print(\"\\n\\nERROR METRICS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "def calculate_error_metrics(ground_truth, predictions):\n",
    "    \"\"\"Calculate error-based metrics\"\"\"\n",
    "    errors = {}\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    errors['mae'] = mean_absolute_error(ground_truth, predictions)\n",
    "    \n",
    "    # Root Mean Square Error\n",
    "    errors['rmse'] = np.sqrt(mean_squared_error(ground_truth, predictions))\n",
    "    \n",
    "    # Mean error (bias)\n",
    "    errors['mean_error'] = np.mean(predictions - ground_truth)\n",
    "    \n",
    "    # Percentage of exact matches\n",
    "    errors['exact_match'] = np.mean(ground_truth == predictions) * 100\n",
    "    \n",
    "    # Percentage within 1 point\n",
    "    errors['within_1'] = np.mean(np.abs(ground_truth - predictions) <= 1) * 100\n",
    "    \n",
    "    return errors\n",
    "\n",
    "errors_a = calculate_error_metrics(expert_ratings, judge_a_ratings)\n",
    "errors_b = calculate_error_metrics(expert_ratings, judge_b_ratings)\n",
    "\n",
    "error_df = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Root Mean Square Error', 'Mean Error (Bias)', \n",
    "               'Exact Match %', 'Within ±1 Point %'],\n",
    "    'Judge A': [\n",
    "        f\"{errors_a['mae']:.3f}\",\n",
    "        f\"{errors_a['rmse']:.3f}\",\n",
    "        f\"{errors_a['mean_error']:+.3f}\",\n",
    "        f\"{errors_a['exact_match']:.1f}%\",\n",
    "        f\"{errors_a['within_1']:.1f}%\"\n",
    "    ],\n",
    "    'Judge B': [\n",
    "        f\"{errors_b['mae']:.3f}\",\n",
    "        f\"{errors_b['rmse']:.3f}\",\n",
    "        f\"{errors_b['mean_error']:+.3f}\",\n",
    "        f\"{errors_b['exact_match']:.1f}%\",\n",
    "        f\"{errors_b['within_1']:.1f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Error Analysis:\")\n",
    "print(error_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-judge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
